{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b260b8cb",
   "metadata": {},
   "source": [
    "# Notebook 2 — Extraction des embeddings faciaux\n",
    "\n",
    "## Objectif\n",
    "Ce notebook a pour but de transformer chaque visage prétraité en un vecteur numérique appelé *embedding facial*.\n",
    "\n",
    "Ces embeddings sont extraits à l’aide d’un modèle de reconnaissance faciale pré-entraîné.\n",
    "Aucun entraînement n’est réalisé ici : le modèle est utilisé uniquement en mode *feature extractor*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014bcc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import InceptionResnetV1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b0faca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"../data\")\n",
    "\n",
    "PROC_FAIRFACE = BASE_DIR / \"processed\" / \"fairface_faces\"\n",
    "PROC_OUR_FACES = BASE_DIR / \"processed\" / \"our_faces\"\n",
    "\n",
    "OUT_DIR = BASE_DIR / \"processed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b0d829",
   "metadata": {},
   "source": [
    "## Chargement du modèle pré-entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff23a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé sur : cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = InceptionResnetV1(pretrained=\"vggface2\").to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Modèle chargé sur :\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405fc9e6",
   "metadata": {},
   "source": [
    "## Prétraitement des images pour le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d541781",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((160,160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c1ac9",
   "metadata": {},
   "source": [
    "## Fonction d’extraction d’un embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94896668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    x = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model(x)\n",
    "\n",
    "    emb = emb.cpu().numpy().flatten()\n",
    "    emb = emb / np.linalg.norm(emb)  # normalisation L2\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da71ec",
   "metadata": {},
   "source": [
    "## Extraction des embeddings FairFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a495a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairface_embeddings = []\n",
    "fairface_paths = []\n",
    "\n",
    "for img_path in PROC_FAIRFACE.glob(\"*.jpg\"):\n",
    "    emb = extract_embedding(img_path)\n",
    "    fairface_embeddings.append(emb)\n",
    "    fairface_paths.append(img_path.name)\n",
    "\n",
    "fairface_embeddings = np.vstack(fairface_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4aea64",
   "metadata": {},
   "source": [
    "## Extraction des embeddings des photos personnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "370418c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_embeddings = []\n",
    "our_meta = []\n",
    "\n",
    "for person_dir in PROC_OUR_FACES.iterdir():\n",
    "    if not person_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    for img_path in person_dir.glob(\"*.jpg\"):\n",
    "        emb = extract_embedding(img_path)\n",
    "        our_embeddings.append(emb)\n",
    "        our_meta.append({\n",
    "            \"identity\": person_dir.name,\n",
    "            \"filename\": img_path.name\n",
    "        })\n",
    "\n",
    "our_embeddings = np.vstack(our_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caded39",
   "metadata": {},
   "source": [
    "## Vérification des dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d43d2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings FairFace : (10530, 512)\n",
      "Embeddings photos personnelles : (21, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings FairFace :\", fairface_embeddings.shape)\n",
    "print(\"Embeddings photos personnelles :\", our_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef4774",
   "metadata": {},
   "source": [
    "## Sauvegarde des embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13359f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings sauvegardés.\n"
     ]
    }
   ],
   "source": [
    "np.save(OUT_DIR / \"embeddings_fairface.npy\", fairface_embeddings)\n",
    "np.save(OUT_DIR / \"embeddings_our_faces.npy\", our_embeddings)\n",
    "\n",
    "print(\"Embeddings sauvegardés.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
